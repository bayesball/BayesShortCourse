---
title: "Bayesian Thinking: Fundamentals, Regression and Multilevel Modeling"
author: "Jim Albert and Monika Hu"
date: 1/9/2023
format: 
  beamer:
    theme: AnnArbor
    colortheme: beaver
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                  message = FALSE, 
                  warning = FALSE)

require(ggplot2)
require(gridExtra)
require(ProbBayes)
require(tidyverse)
crcblue <- "#2905a1"
knitr::opts_chunk$set(echo = TRUE)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

## Webinar 1-3: Regression Models for Continuous Data 

\tableofcontents[hideallsubsections]


# Introduction: adding a continuous predictor variable

## Review: the normal model

- When you have continuous outcomes, you can use a normal model:
\begin{equation}
Y_i \mid \mu, \sigma \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma), \,\,\, i = 1, \cdots, n.
\end{equation}



- Suppose now you have another continuous variable available, $x_i$. And you want to use the information in $x_i$ to learn about $Y_i$.
    1. $Y_i$ is the log of expenditure of CU's
    2. $x_i$ is the log of total income of CU's
    
- Is the model in Equation (1) flexible to include $x_i$?


## An observation specific mean

- We can adjust the model in Equation (1) to Equation (2), where the common mean $\mu$ is replaced by an observation specific mean $\mu_i$:
\begin{equation}
Y_i \mid \mu_i, \sigma \overset{ind}{\sim} \textrm{Normal}(\mu_i, \sigma), \,\,\, i = 1, \cdots, n.
\end{equation}

- How to link $\mu_i$ and $x_i$?


## Linear relationship between the mean and the predictor

- One basic approach: use a linear relationship:
\begin{equation}
\mu_i = \beta_0 + \beta_1 x_i, \,\,\, i = 1, \cdots, n.
\end{equation}

- $x_i$'s are known constants.

- $\beta_0$ (intercept) and $\beta_1$ (slope) are unknown parameters.

- Bayesian approach: 

    1. assign a prior distribution to $(\beta_0, \beta_1, \sigma)$
    2. perform inference
    3. summarize posterior distribution of these parameters
    

## The simple linear regression model

- To put everything together, a linear regression model:
\begin{equation}
Y_i \mid x_i, \beta_0, \beta_1, \sigma \overset{ind}{\sim} \textrm{Normal}(\beta_0 + \beta_1 x_i, \sigma), \,\,\, i = 1, \cdots, n.
\end{equation}


#\includegraphics[scale=0.25]{figures/Regression_View}
![](figures/Regression_View.pdf){width=40%}


## The simple linear regression model cont'd

```{r fig.height = 3, fig.width = 3, fig.align = "center", size = "footnotesize", echo = FALSE}
CEData <- read.csv("CEsample_regression.csv", header = T, sep = ",")
ggplot(CEData, aes(x = log_TotalIncome, y = log_TotalExp)) +
  geom_point(size=1, color = crcblue) + 
  labs(x = "log(Income)", y = "log(Expenditure)") +
  theme_grey(base_size = 10, base_family = "") 
```


# A simple linear regression for the CE sample

## The CE sample

The CE sample comes from the 2017 Q1 CE PUMD: 4 variables, 994 observations.
\begin{table}[htb]
\begin{center}
\begin{tabular}{ll} 
\hline
Variable & Description  \\ \hline
log(Expenditure) & Continuous; CU's total expenditures in last \\
& quarter (log)\\
log(Income) & Continuous; the amount of CU income before taxes in \\
& past 12 months (log)\\
Rural & Binary; the urban/rural status of CU: 0 = Urban, \\
& 1 = Rural\\ 
Race & Categorical; the race category of the reference person: \\
& 1 = White, 2 = Black, 3 = Native American, \\
& 4 = Asian, 5 = Pacific Islander, 6 = Multi-race \\ \hline 
\end{tabular}
\end{center}
\label{default}
\end{table}



## An SLR for the CE sample

- For now, we focus on a simple linear regression:
\begin{eqnarray}
Y_i \mid \mu_i, \sigma &\overset{ind}{\sim}& \textrm{Normal}(\mu_i, \sigma), \\
\mu_i &=& \beta_0 + \beta_1 x_i.
\end{eqnarray}


\begin{table}[htb]
\begin{center}
\begin{tabular}{ll} 
\hline
Variable & Description  \\ \hline
log(Expenditure) & Continuous; CU's total expenditures in last \\
& quarter (log)\\
log(Income) & Continuous; the amount of CU income before\\
&  taxes in past 12 months (log)\\ \hline
\end{tabular}
\end{center}
\label{default}
\end{table}

## A weakly informative prior

- Assume know little about $(\beta_0, \beta_1, \sigma)$.

- Assuming independence: $g(\beta_0, \beta_1, \sigma) = g(\beta_0) g(\beta_1) g(\sigma)$.

- For example:
\begin{eqnarray*}
\beta_0 &\sim& \textrm{Normal}(0, 10),\\
\beta_1 &\sim& \textrm{Normal}(0, 10),\\
\sigma &\sim& \textrm{Cauchy}(0, 1).
\end{eqnarray*}



## Fitting the model

- Use the \texttt{brm()} function with \texttt{family = gaussian}.

```{r, size = "footnotesize", warning = FALSE, message = FALSE, results = 'hide'}
library(brms)
SLR_fit <- brm(data = CEData, family = gaussian,
               log_TotalExp ~ 1 + log_TotalIncome,
               prior = c(prior(normal(0, 10), class = Intercept),
                         prior(normal(0, 10), class = b),
                         prior(cauchy(0, 1), class = sigma)),
               iter = 10000, warmup = 8000, chains = 2, seed = 123)
```

## Saving posterior draws

- Save \texttt{post} as a matrix of simulated posterior draws

```{r, size = "footnotesize", warning = FALSE, message = FALSE}
post <- as_draws_df(SLR_fit)
head(post)
```

## Posterior plots

- Function \texttt{mcmc\_areas()} displays a density estimate of the simulated posterior draws with a specified credible interval.

```{r fig.height = 2, fig.width = 3, fig.align = "center", size = "footnotesize"}
library(bayesplot)
mcmc_areas(post, pars = "b_log_TotalIncome", prob = 0.95)
```

## Posterior plots cont'd

- Function \texttt{mcmc\_scatter()} creates a simple scatterplot of two parameters. 

```{r fig.height = 2, fig.width = 2, fig.align = "center", size = "footnotesize"}
mcmc_scatter(post, pars = c("b_Intercept", "b_log_TotalIncome"))
```

## Plotting posterior inference against the data

- Plot the first 10 $(\beta_0, \beta_1)$ fits to the data

```{r fig.height = 3, fig.width = 3, fig.align = "center", size = "footnotesize", echo = FALSE}
ggplot(data =  CEData, 
         aes(x = log_TotalIncome, y = log_TotalExp)) +
  geom_abline(intercept = post$b_Intercept[1:10], 
              slope     = post$b_log_TotalIncome[1:10],
              size = 1/3, alpha = .3) +
  geom_point(color = crcblue) +
  coord_cartesian(xlim = range(CEData$log_TotalIncome),
                  ylim = range(CEData$log_TotalExp)) +
  theme(panel.grid = element_blank())
```

## Predictions

- Use the \texttt{predict()} function to make predictions of observed CUs.

```{r size = "footnotesize"}
pred_logExp_obs <- predict(SLR_fit, newdata = CEData)
head(pred_logExp_obs)
```

## Predictions cont'd

- If we focus on one CU, i.e.g CU 1; set \texttt{summary = FALSE} to obtain predicted values.

```{r size = "footnotesize"}
pred_logExp_obs_1 <- predict(SLR_fit, newdata = CEData[1, ], 
                             summary = FALSE)
```

```{r fig.height = 2.5, fig.width = 2.5, fig.align = "center", size = "footnotesize", echo = FALSE}
df <- data.frame(pred = pred_logExp_obs_1)
ggplot(df, aes(x = pred)) +
  geom_density() +
  geom_vline(xintercept = CEData[1, "log_TotalExp"], size = 1.5, color = "red") + xlab("Predicted values for CU 1") + 
  annotate(geom = "text", x = 8, y = 0.5,
           label = "log_TotalExp of CU 1",
           size = 2) 
```

## Predictions cont'd

- Now suppose we get to know a new CU with log\_TotalIncome = 10, and we want to predict its log\_TotalExp

```{r size = "footnotesize"}
newdata <- data.frame(log_TotalIncome = c(10))
pred_logExp_new <- predict(SLR_fit, newdata = newdata)
pred_logExp_new
```

## Model checking

- Function \texttt{pp\_check()} performs posterior predictive checks
    - plot density estimates for 10 replicated samples from the posterior predictive distribution and overlay the observed log income distribution

```{r fig.height = 2, fig.width = 3, fig.align = "center", size = "footnotesize"}
pp_check(SLR_fit)
```


# A multiple linear regression for the CE sample

## Adding a binary predictor

\begin{table}[htb]
\begin{center}
\begin{tabular}{ll} 
\hline
Variable & Description  \\ \hline
log(Expenditure) & Continuous; CU's total expenditures in last \\
& quarter (log)\\
Rural & \textcolor{red}{Binary}; the urban/rural status of CU: 0 = Urban, \\
& 1 = Rural\\ \hline 
\end{tabular}
\end{center}
\label{default}
\end{table}


- Consider Rural as a binary categorical variable to classify two groups:
    - The urban group
    - The rural group
    
- Such classification puts an emphasis on the \textcolor{red}{difference of the expected outcomes} between the two groups.


## With only one binary predictor

- For simplicity, consider a simplified regression model with a single predictor: the binary indicator for rural area $x_i$.

\begin{equation}
\mu_i = \beta_0 + \beta_1 x_i  = 
 \begin{cases}
  \beta_0, & \text{ the urban group}; \\
  \beta_0 + \beta_1, & \text{ the rural group}.  \\
  \end{cases}
\end{equation}

- The expected outcome $\mu_i$ for CUs in the urban group: $\beta_0$.
- The expected outcome $\mu_i$ for CUs in the rural group: $\beta_0 + \beta_1$.
- $\beta_1$ represents the \textcolor{red}{change in the expected outcome} $\mu_i$ from the urban group to the rural group.


## The multiple linear regression model

\begin{eqnarray}
Y_i \mid \mu_i, \sigma &\overset{ind}{\sim}& \textrm{Normal}(\mu_i, \sigma), \\
\mu_i &=& \beta_0 + \beta_1 x_{i, logIncome} + \beta_2 x_{i, Rural}.
\end{eqnarray}


## A weakly informative prior

- Assume know little about $(\beta_0, \beta_1, \beta_2, \sigma)$.

\begin{eqnarray*}
\beta_0 &\sim& \textrm{Normal}(0, 10),\\
\beta_1 &\sim& \textrm{Normal}(0, 10),\\
\beta_2 &\sim& \textrm{Normal}(0, 10),\\
\sigma &\sim& \textrm{Cauchy}(0, 1).
\end{eqnarray*}

## Fitting the model
- Use the \texttt{brm()} function with \texttt{family = gaussian}.

- Use \texttt{as.factor()} for binary / categorical predictors.

```{r, size = "footnotesize", warning = FALSE, message = FALSE, results = 'hide'}
MLR_fit <- brm(data = CEData, family = gaussian,
               log_TotalExp ~ 1 + log_TotalIncome + as.factor(Rural),
               prior = c(prior(normal(0, 10), class = Intercept),
                         prior(normal(0, 10), class = b),
                         prior(cauchy(0, 1), class = sigma)),
               iter = 10000, warmup = 8000, chains = 2, seed = 123)
```

## Saving posterior draws

- Save \texttt{post} as a matrix of simulated posterior draws

```{r, size = "footnotesize", warning = FALSE, message = FALSE}
post_MLR <- as_draws_df(MLR_fit)
head(post_MLR)
```

## Posterior plots

- Function \texttt{mcmc\_areas()} displays a density estimate of the simulated posterior draws with a specified credible interval.

```{r fig.height = 2, fig.width = 3, fig.align = "center", size = "footnotesize"}
mcmc_areas(post_MLR, 
           pars = c("b_Intercept", "b_log_TotalIncome", 
                    "b_as.factorRural1"), 
           prob = 0.95)
```


## Predictions

- Use the \texttt{predict()} function to make predictions of observed CUs.

```{r size = "footnotesize"}
pred_logExp_obs <- predict(MLR_fit, newdata = CEData)
head(pred_logExp_obs)
```

## Predictions cont'd

- Now suppose we get to know two new CU with log\_TotalExp = 10, one is rural and the other is urban, and we want to predict its log\_TotalIncome.

- Can also use the \texttt{posterior\_predict()} function.

```{r size = "footnotesize"}
newdata <- data.frame(log_TotalIncome = c(10, 10), Rural = c(1, 0))
pred_logExp_new <- posterior_predict(MLR_fit, newdata = newdata)
apply(pred_logExp_new, 2, summary)
```

## Model checking

- Function \texttt{pp\_check()} plots density estimates for 10 replicated samples from the posterior predictive distribution and overlay the observed log income distribution.

```{r fig.height = 2, fig.width = 3, fig.align = "center", size = "footnotesize"}
pp_check(MLR_fit)
```

# Wrap-up and additional material

## Wrap-up

- Bayesian linear regression:
    - Linear relationship between the expected outcome and the predictor(s)
    - Continuous predictors, binary predictors
    - Using the \texttt{brms} package; prior choices

- Bayesian inferences
    - Bayesian hypothesis testing and credible interval
    - Bayesian prediction
    - Posterior predictive checks


## Additional material: adding a categorical predictor


\begin{table}[htb]
\begin{center}
\begin{tabular}{ll} 
\hline
Variable & Description  \\ \hline
log(Expenditure) & Continuous; CU's total expenditures in last \\
& quarter (log)\\
Race & {\color{red}Categorical}; the race category of the reference person: \\
& 1 = White, 2 = Black, 3 = Native American, \\
& 4 = Asian, 5 = Pacific Islander, 6 = Multi-race \\ \hline 
\end{tabular}
\end{center}
\label{default}
\end{table}


- It is common to consider it as a categorical variable to classify multiple groups:
    - How many groups? What are the groups?
    
- Such classification puts an emphasis on the \textcolor{red}{difference of the expected outcomes} between one group to \textcolor{red}{the reference group}.



## With only one categorical predictor

- For simplicity, consider a simplified regression model with a single predictor: the race category of the reference person $x_i$.

\begin{eqnarray}
\mu_i &=& \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \beta_3 x_{i,3} + \beta_4 x_{i,4}  + \beta_5 x_{i,5} \nonumber \\
&=& 
 \begin{cases}
  \beta_0, & \text{ White}; \\
  \beta_0 + \beta_1, & \text{ Black};  \\
  \beta_0 + \beta_2, & \text{ Native American};  \\
  \beta_0 + \beta_3, & \text{ Asian};  \\
  \beta_0 + \beta_4, & \text{ Pacific Islander};  \\
  \beta_0 + \beta_5, & \text{ Multi-race}.  \\
  \end{cases}
\end{eqnarray}

- What is the expected outcome $\mu_i$ for CUs in the White group?

- What is the expected outcome $\mu_i$ for CUs in the Asian group?

- What does $\beta_5$ represent?
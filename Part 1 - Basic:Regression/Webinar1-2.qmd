---
title: "Bayesian Thinking: Fundamentals, Regression and Multilevel Modeling"
author: "Jim Albert and Monika Hu"
date: 1/9/2023
format: 
  beamer:
    theme: AnnArbor
    colortheme: beaver
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                  message = FALSE, 
                  warning = FALSE)

require(ggplot2)
require(gridExtra)
require(ProbBayes)
require(tidyverse)
crcblue <- "#2905a1"
knitr::opts_chunk$set(echo = TRUE)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

## Webinar 1-2: Normal Models for Continuous Data 

\tableofcontents[hideallsubsections]

# Example: Expenditures in the CE and normal distribution

## The Consumer Expenditure Surveys data (CE)
- Conducted by the U.S. Census Bureau for the BLS.

- Contains data on expenditures, income, and tax statistics about consumer units (CU) across the country.

- Provides information on the buying habits of U.S. consumers.

\pause

- We work with PUMD micro-level data, with the continuous variable \textcolor{red}{TOTEXPPQ}: CU total expenditures last quarter.

- We work with Q1 2017 sample: $n = 6,208$.


## The Total Expenditure variable

```{r message = FALSE}
library(readr)
CEsample <- read_csv("CEsample.csv")

summary(CEsample$TotalExpLastQ)
sd(CEsample$TotalExpLastQ)
```

## The Total Expenditure variable cont'd

```{r fig.height = 2.5, fig.width = 2.5, fig.align = "center", size = "footnotesize", echo = FALSE}
ggplot(data = CEsample, aes(TotalExpLastQ)) +
  geom_density(color = crcblue, size = 1) + 
  labs(title = "Total expenditure last Q") +
  theme_grey(base_size = 8, base_family = "") 
```

- Very skewed to the right.

- Take log and transform it to the log scale.


## Log transformation of the Total Expenditure variable

```{r message = FALSE}
CEsample$LogTotalExpLastQ <- log(CEsample$TotalExpLastQ)
```

```{r fig.height = 2.2, fig.width = 2.2, fig.align = "center", size = "footnotesize", echo = FALSE}
ggplot(data = CEsample, aes(LogTotalExpLastQ)) +
  geom_density(color = crcblue, size = 1) + 
  labs(title = "Total expenditure last Q (log)") +
  theme_grey(base_size = 8, base_family = "") 
```


## The normal distribution

- The normal distribution is a symmetric, bell-shaped distribution.

- It has two parameters: mean $\mu$ and standard deviation $\sigma$.


- The probability density function (pdf) of $\textrm{Normal}(\mu, \sigma)$ is:
$$
f(y) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(\frac{-(y - \mu)^2}{2 \sigma^2}\right), -\infty < y < \infty.
$$

## The normal distribution cont'd

```{r fig.height = 3, fig.width = 5, fig.align = "center", size = "footnotesize", echo = FALSE}
ggplot(data = data.frame(y = c(-5, 5)), aes(y)) + 
  stat_function(fun = dnorm, args = list(mean = 0, sd = 0.5), aes(color = "Normal(0, 0.5)")) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = "Normal(0, 1)")) + 
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2), aes(color = "Normal(0, 2)")) +
  stat_function(fun = dnorm, args = list(mean = -2, sd = 0.5), aes(color = "Normal(-2, 0.5)")) +
  ylab("f(y)")
```


## $i.i.d.$ normals

- Suppose there are a sequence of $n$ responses: $Y_1, Y_2, \cdots, Y_n$.

- Further suppose each response \textcolor{red}{independently and identically} follows a normal distribution:

$$
Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma).
$$


-  Then the joint probability density function (joint pdf) of $y_1, \cdots, y_n$ is:

$$
f(y_1, \cdots, y_n) = \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(\frac{-(y_i - \mu)^2}{2 \sigma^2}\right), -\infty < y_i < \infty.
$$


## Recap from beta-binomial


- Bayesian inference procedure:
    - The prior distribution: $p \sim \textrm{Beta}(\alpha, \beta)$
    - The sampling density: $Y \sim \textrm{Binomial}(N, p)$
    - The posterior distribution: $p \mid Y \sim \textrm{Beta}(\alpha + Y, \beta + N - Y)$
    
\pause

- What to do for a normal model $Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma)$?
    - Data model/sampling density is chosen: normal.
    - What to do with two parameters $\mu$ and $\sigma$?
    - How to specify priors?
    

# Conjugate prior and posterior inferences for $\mu$ 


## Overview

- The data model/sampling density for $N$ observations:
$$
Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma).
$$

- There are two parameters $\mu$ and $\sigma$ in the normal model.

- Need a joint prior distribution (if both $\mu$ and $\sigma$ are unknown):
$$
g(\mu, \sigma).
$$

- Bayes' rule will help us derive a joint posterior:
$$
g(\mu, \sigma \mid Y_1, \cdots, Y_n) \propto g(\mu, \sigma) f(Y_1, \cdots, Y_N \mid \mu, \sigma)
$$


## If only mean $\mu$ is unknown

- Special case: \textcolor{red}{$\mu$ is unknown, $\sigma$ is known}.

- There is only one parameter $\mu$ in $Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma)$.

- The Bayesian inference procedure simplifies to:
    - The data model for $N$ observations with \textcolor{red}{$\sigma$ known}:
    $$
    Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma).
    $$
    
    - Need a prior distribution for $\mu$:
    $$
    g(\mu \mid \textcolor{red}{\sigma}).
    $$
    
    - Bayes' rule will help us derive a posterior for $\mu$:
    $$
    g(\mu \mid Y_1, \cdots, Y_N, \textcolor{red}{\sigma}) \propto g(\mu \mid \textcolor{red}{\sigma}) f(Y_1, \cdots, Y_N \mid \mu, \textcolor{red}{\sigma}).
    $$


## Normal conjugate prior

- For this special case, normal prior for $\mu$ is a conjugate prior:
    - The prior distribution:
    $$
    \mu \mid \textcolor{red}{\sigma} \sim \textrm{Normal}(\mu_0, \sigma_0).
    $$
    
    - The sampling density: 
    $$
    Y_1, \cdots, Y_N \mid \mu, \sigma \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma).
    $$
    
    \pause
    
    - The posterior distribution: 
    $$
    \mu \mid Y_1, \cdots, Y_N,\textcolor{red}{\phi} \sim \textrm{Normal}\left(\frac{\phi_0 \mu_0 + N\phi\bar{Y} }{\phi_0 + N\phi}, \sqrt{\frac{1}{\phi_0 + N \phi}}\right),
    $$
    where $\phi = \frac{1}{\sigma^2}$ (and $\phi_0 = \frac{1}{\sigma_0^2}$), the precision. \textcolor{red}{Since $\sigma$ (and $\sigma_0$) is known, $\phi$ (and $\phi_0$) is known too.}



## Example on log(Total Expenditure)


- Prior for $\mu$ is $\mu \sim \textrm{Normal}(5, 1)$, i.e. $\mu_0 = 5, \phi_0 = 1$

- Our log(Total Expenditure): $N = 6208$, $\bar{Y} = 8.75$

- Assume $\phi = 1.25$, i.e. $\sigma = \sqrt{1/1.25}$

- Use these quantities to obtain posterior for $\mu$:
  $$
    \mu \mid Y_1, \cdots, Y_N, \phi \sim \textrm{Normal}\left(\frac{\phi_0 \mu_0 + N\phi\bar{Y} }{\phi_0 + N\phi}, \sqrt{\frac{1}{\phi_0 + N \phi}}\right).
  $$


## Posterior for $\mu$

```{r size = "footnotesize"}
mu_0 <- 5
sigma_0 <- 1
phi_0 <- 1/sigma_0^2
ybar <- mean(CEsample$LogTotalExpLastQ)
phi <- 1.25
n <- dim(CEsample)[1]
mu_n <- (phi_0*mu_0+n*ybar*phi)/(phi_0+n*phi)
sd_n <- sqrt(1/(phi_0+n*phi))

mu_n
sd_n
```

## Posterior for $\mu$ cont'd

```{r fig.height = 3, fig.width = 5, fig.align = "center", size = "footnotesize", echo = FALSE}
par1 <- c(mu_0, sigma_0)
par2 <- c(mu_n, sd_n)
ggplot(data.frame(x = c(0, 25)), aes(x)) +
  stat_function(fun = dnorm, size = 1,
                linetype = "dashed",
                color = crcblue,
                args = list(mean = par1[1], sd = par1[2])) +
  stat_function(fun = dnorm, size = 1,
                color = crcblue,
                args = list(mean = par2[1], sd = par2[2])) +
  increasefont() +
  xlab(expression(mu))  +
  ylab("") +
  annotate(geom = "text", x = 5, y = 2,
           label = "Prior", size = 5) +
  annotate(geom = "text", x = 12, y = 20,
           label = "Posterior", size = 5)
```

## Bayesian inferences: hypothesis testing

- Suppose someone thinks the log(Total Expenditure) of CUs in the U.S. on average is at least \$8.5 (i.e. \$4914), is this statement reasonable?

- Exact solution:

```{r, size = "footnotesize"}
1 - pnorm(8.5, mean = mu_n, sd = sd_n)
```

- Monte Carlo simulation solution:

```{r, size = "footnotesize"}
set.seed(123)
S <- 1000
mu_post <- rnorm(S, mean = mu_n, sd = sd_n)
sum(mu_post >= 8.5) / S 
```


## Bayesian inferences: credible interval

- Bayesian credible interval: an interval contains the unknown parameter with a certain probability.

- What is a 95\% credible interval for $\mu$?

- Exact solution:

```{r, size = "footnotesize"}
qnorm(c(0.025, 0.975), mean = mu_n, sd = sd_n)
```

- Monte Carlo simulation solution:

```{r, size = "footnotesize"}
quantile(mu_post, c(0.025, 0.975))
```

## Bayesian inferences: prediction

- Suppose we are interested in predicting log(Total Expenditure) of another CU.

- The posterior predictive distribution is

\begin{equation}
f(Y^* \mid Y_1, \cdots, Y_N) =  \int f(Y^* \mid \mu, \sigma) g(\mu \mid Y_1, \cdots, Y_N, \sigma) d \mu.
\label{eq:NormalPred}
\end{equation}

- The integration step in Equation (\ref{eq:NormalPred}) can be approximated through simulation.
    - Step 1: Sample a value of $\mu$ from its posterior distribution
    $$
    \mu \mid Y_1, \cdots, Y_N, \phi \sim \textrm{Normal}\left(\frac{\phi_0 \mu_0 + N\phi\bar{Y} }{\phi_0 + N\phi}, \sqrt{\frac{1}{\phi_0 + N \phi}}\right).
    $$
    - Step 2: Sample a new observation $Y^*$ from the sampling model
    $$
    Y^* \sim \textrm{Normal}(\mu, \sigma)
    $$

## Bayesian inferences: prediction cont'd

```{r, size = "footnotesize"}
set.seed(123)
S <- 1000
mu_post <- rnorm(S, mean = mu_n, sd = sd_n)
y_pred <- rnorm(S, mean = mu_post, sd = sqrt(1 / phi))
```

```{r fig.height = 2.2, fig.width = 2.2, fig.align = "center", size = "footnotesize", echo = FALSE}
df <- data.frame(y_pred)
names(df) <- c("prediction")
ggplot(df, aes(prediction)) +
  geom_density(color = crcblue, size = 1) + 
  labs(title = "Density plot of predictions") + xlab("Y") +
  theme_grey(base_size = 8, base_family = "")
```



## Using Stan

- Write a Stan script defining the Bayesian model.

```
data {
  int<lower=0> N;  // number of observations
  real y[N];   // vector of continuous observations
}
parameters {
  real mu; // mean parameter
}
model {
  mu ~ normal(5, 1);  // prior 
  for (i in 1:N) {
      y[i] ~ normal(mu, sqrt(1 / 1.25)); // observation model 
  }
}
```

# Inferences for $\mu$ and $\sigma$ 

## Overview

- The data model/sampling density for $N$ observations:
$$
Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma).
$$

- There are two parameters $\mu$ and $\sigma$ in the normal model.

- Need a joint prior distribution (if both $\mu$ and $\sigma$ are unknown):
$$
g(\mu, \sigma).
$$

- Bayes' rule will help us derive a joint posterior:
$$
g(\mu, \sigma \mid Y_1, \cdots, Y_n) \propto g(\mu, \sigma) f(Y_1, \cdots, Y_N \mid \mu, \sigma)
$$

## Priors for $\mu$ and for $\sigma$

- Suppose we keep the normal prior for $\mu$:

$$
\mu \sim \textrm{Normal}(\mu_0, \sigma_0).
$$

- Now let's also specify a prior for $\sigma > 0$:

$$
\sigma \sim \textrm{Cauchy}(\gamma_1, \gamma_2).
$$

- We know that from Bayes' rule, we obtain our joint posterior:
$$
g(\mu, \sigma \mid Y_1, \cdots, Y_n) \propto g(\mu) g(\sigma) f(Y_1, \cdots, Y_N \mid \mu, \sigma).
$$

## Markov chain Monte Carlo methods

- Our goal is to estimate the joint posterior:

$$
g(\mu, \sigma \mid Y_1, \cdots, Y_n).
$$

- As an approximation, we can iterate by sampling: 
    - Sample $\mu$ at iteration $i$:
    $$\mu^{(i)} \sim g(\mu \mid Y_1, \cdots, Y_N, \sigma^{(i-1)}).$$
    
    - Sample $\sigma$ at iteration $i$:
    $$\sigma^{(i)} \sim g(\sigma \mid Y_1, \cdots, Y_N, \mu^{(i)}).$$

- After convergence, $\{\mu^{(1)}, \cdots, \mu^{(S)}\}$ and $\{\sigma^{(1)}, \cdots, \sigma^{(S)}\}$ serve as approximations to the posterior distribution.

## Using Stan

- Write a Stan script defining the Bayesian model.

```
data {
  int<lower=0> N;  // number of observations
  real y[N];   // vector of continuous observations
}
parameters {
  real mu; // mean parameter
  real<lower=0> sigma; // sd parameter
}
model {
  mu ~ normal(5, 1);  // prior for mu
  sigma ~ cauchy(0, 1); // prior for sigma
  for (i in 1:N) {
      y[i] ~ normal(mu, sigma); // observation model 
  }
}
```

## Run Stan using the \texttt{rstan} package

- Enter data by a list
```{r, size = "footnotesize"}
n <- dim(CEsample)[1]
my_data <- list(N = n, y = CEsample$LogTotalExpLastQ)
```

- Inputs are Stan model file and the data list.
```{r, size = "footnotesize", warning = FALSE, message = FALSE, results = 'hide'}
library(rstan)
fit_normal <- stan(file = "normal_2unknowns.stan",
                          data = my_data,
                          refresh = 0)
```

## Extract the posterior draws

```{r, size = "footnotesize"}
draws <- as.data.frame(fit_normal)
head(draws)
```

## Histogram of the simulated draws of $\mu$

```{r, fig.height = 2, fig.width = 3, size = "footnotesize"}
library(bayesplot)
mcmc_hist(draws, pars = 'mu')
```

## Histogram of the simulated draws of $\sigma$

```{r, fig.height = 2, fig.width = 3, size = "footnotesize"}
mcmc_hist(draws, pars = 'sigma')
```

## Posterior summaries

```{r, size = "footnotesize"}
summary(fit_normal)
```

## Bayesian inferences: hypothesis testing and credible interval

- Since exact posterior distribution is not available, our inferential methods are mainly Monte Carlo simulation.

- Hypothesis testing: $\mu$ at least \$8.5?

```{r, size = "footnotesize"}
sum(draws$mu > 8.5) / dim(draws)[1]
```

- Credible interval: a 95\% credible interval for $\sigma$?

```{r, size = "footnotesize"}
quantile(draws$sigma, c(0.025, 0.975))
```

## Bayesian inferences: prediction

$$
Y^* \sim \textrm{Normal}(\mu, \sigma)
$$

```{r, size = "footnotesize"}
set.seed(123)
S <- dim(draws)[1]
y_pred2 <- rnorm(S, draws$mu, draws$sigma)
```

```{r fig.height = 2.2, fig.width = 2.2, fig.align = "center", size = "footnotesize", echo = FALSE}
df <- data.frame(y_pred2)
names(df) <- c("prediction")
ggplot(df, aes(prediction)) +
  geom_density(color = crcblue, size = 1) + 
  labs(title = "Density plot of predictions") + xlab("Y") +
  theme_grey(base_size = 8, base_family = "")
```

# Wrap-up and additional material

## Wrap-up

- Bayesian inference procedure:
    - Step 1: express an opinion about the location of the parameters before sampling (prior).
    - Step 2: take the sample (data/likelihood).
    - Step 3: use Bayes' rule to sharpen and update the previous opinion about the parameters given the information from the sample (posterior).
    

- Bayesian inferences
    - Bayesian hypothesis testing
    - Bayesian credible interval
    - Bayesian prediction


## Additional material: posterior predictive checks

- A way to check model fitting

- Sample $S$ copies of predictions of the same sample size as the original data

```{r, size = "footnotesize"}
set.seed(123)
S <- dim(draws)[1]
sim_ytilde <- function(j){
  rnorm(n, draws$mu, draws$sigma)
}
ytilde <- t(sapply(1:S, sim_ytilde))
```

## Posterior predictive checks cont'd

- Use some statistics to check, e.g. the average

```{r, size = "footnotesize"}
pred_ybar_sim <- apply(ytilde, 1, mean)
```

```{r fig.height = 2.2, fig.width = 2.2, fig.align = "center", size = "footnotesize", echo = FALSE}
ggplot(data.frame(Ybar = pred_ybar_sim),
       aes(Ybar)) +
  geom_density(size = 1.5, color = crcblue) +
  geom_vline(xintercept = ybar, size = 1.5, color = "red") + xlab("Ybar predicted") + 
  annotate(geom = "text", x = 8.77, y = 32,
           label = "Ybar from data",
           size = 2) 
```
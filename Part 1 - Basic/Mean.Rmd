---
title: 'Bayesian Thinking:  Fundamentals, Regression and Multilevel Modeling'
author: "Jim Albert and Jingchen (Monika) Hu"
date: "November 2020"
output:
  beamer_presentation:
    theme: AnnArbor
    colortheme: beaver
    fonttheme: structurebold
    includes:
      in_header: header_pagenrs.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                  message = FALSE, 
                  warning = FALSE)

require(ggplot2)
require(gridExtra)
require(ProbBayes)
require(tidyverse)
crcblue <- "#2905a1"
knitr::opts_chunk$set(echo = TRUE)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

## Outline

\tableofcontents[hideallsubsections]

# Example: Expenditures in the CE and normal distribution

## The Consumer Expenditure Surveys Data (CE)
- Conducted by the U.S. Census Bureau for the BLS.

- Contains data on expenditures, income, and tax statistics about consumer units (CU) across the country.

- Provides information on the buying habits of U.S. consumers.

\pause

- We work with PUMD micro-level data, with the continuous variable \textcolor{red}{TOTEXPPQ}: CU total expenditures last quarter.

- We work with Q1 2017 sample: $n = 6,208$.


## The Total Expenditure variable

```{r message = FALSE}
library(readr)
CEsample <- read_csv("CEsample.csv")

summary(CEsample$TotalExpLastQ)
sd(CEsample$TotalExpLastQ)
```

## The Total Expenditure variable cont'd

```{r fig.height = 2.5, fig.width = 2.5, fig.align = "center", size = "footnotesize", echo = FALSE}
ggplot(data = CEsample, aes(TotalExpLastQ)) +
  geom_density(color = crcblue, size = 1) + 
  labs(title = "Total expenditure last Q") +
  theme_grey(base_size = 8, base_family = "") 
```

- Very skewed to the right.

- Take log and transform it to the log scale.


## Log transformation of the Total Expenditure variable

```{r message = FALSE}
CEsample$LogTotalExpLastQ <- log(CEsample$TotalExpLastQ)
```

```{r fig.height = 2.2, fig.width = 2.2, fig.align = "center", size = "footnotesize", echo = FALSE}
ggplot(data = CEsample, aes(LogTotalExpLastQ)) +
  geom_density(color = crcblue, size = 1) + 
  labs(title = "Total expenditure last Q (log)") +
  theme_grey(base_size = 8, base_family = "") 
```


## The normal distribution

- The normal distribution is a symmetric, bell-shaped distribution.

- It has two parameters: mean $\mu$ and standard deviation $\sigma$.


- The probability density function (pdf) of $\textrm{Normal}(\mu, \sigma)$ is:
$$
f(y) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(\frac{-(y - \mu)^2}{2 \sigma^2}\right), -\infty < y < \infty.
$$

## The normal distribution cont'd

```{r fig.height = 3, fig.width = 5, fig.align = "center", size = "footnotesize", echo = FALSE}
ggplot(data = data.frame(y = c(-5, 5)), aes(y)) + 
  stat_function(fun = dnorm, args = list(mean = 0, sd = 0.5), aes(color = "Normal(0, 0.5)")) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = "Normal(0, 1)")) + 
  stat_function(fun = dnorm, args = list(mean = 0, sd = 2), aes(color = "Normal(0, 2)")) +
  stat_function(fun = dnorm, args = list(mean = -2, sd = 0.5), aes(color = "Normal(-2, 0.5)")) +
  ylab("f(y)")
```


## $i.i.d.$ normals

- Suppose there are a sequence of $n$ responses: $Y_1, Y_2, \cdots, Y_n$.

- Further suppose each response \textcolor{red}{independently and identically} follows a normal distribution:

$$
Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma).
$$


-  Then the joint probability density function (joint pdf) of $y_1, \cdots, y_n$ is:

\begin{equation}
f(y_1, \cdots, y_n) = \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(\frac{-(y_i - \mu)^2}{2 \sigma^2}\right), -\infty < y_i < \infty.
\end{equation}


## Recap from beta-binomial


- Bayesian inference procedure:
    - The prior distribution: $p \sim \textrm{Beta}(\alpha, \beta)$
    - The sampling density: $Y \sim \textrm{Binomial}(N, p)$
    - The posterior distribution: $p \mid Y \sim \textrm{Beta}(a + Y, b + N - Y)$
    
\pause

- What to do for a normal model $Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma)$?
    - Data model/sampling density is chosen: normal.
    - What to do with two parameters $\mu$ and $\sigma$?
    - How to specify priors?
    

# Conjugate prior and posterior inferences for $\mu$ 


## Overview

- The data model/sampling density for $N$ observations:
$$
Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma).
$$

- There are two parameters $\mu$ and $\sigma$ in the normal model.

- Need a joint prior distribution (if both $\mu$ and $\sigma$ are unknown):
\begin{equation}
g(\mu, \sigma).
\end{equation}

- Bayes' rule will help us derive a joint posterior:
\begin{equation}
g(\mu, \sigma \mid y_1, \cdots, y_n) \propto g(\mu, \sigma) f(Y_1, \cdots, Y_N \mid \mu, \sigma)
\end{equation}


## If only mean $\mu$ is unknown

- Special case: \textcolor{red}{$\mu$ is unknown, $\sigma$ is known}.

- There is only one parameter $\mu$ in $Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma)$.

- The Bayesian inference procedure simplifies to:
    - The data model for $N$ observations with \textcolor{red}{$\sigma$ known}:
    $$
    Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma).
    $$
    
    - Need a prior distribution for $\mu$:
    \begin{equation}
    g(\mu \mid \textcolor{red}{\sigma}).
    \end{equation}
    
    - Bayes' rule will help us derive a posterior for $\mu$:
    \begin{equation}
    g(\mu \mid Y_1, \cdots, Y_N, \textcolor{red}{\sigma}) \propto g(\mu \mid \textcolor{red}{\sigma}) f(Y_1, \cdots, Y_N \mid \mu, \textcolor{red}{\sigma}).
    \end{equation}


## Normal conjugate prior

- For this special case, normal prior for $\mu$ is a conjugate prior:
    - The prior distribution:
    \begin{equation}
    \mu \mid \textcolor{red}{\sigma} \sim \textrm{Normal}(\mu_0, \sigma_0).
    \end{equation}
    
    - The sampling density: 
    \begin{equation}
    Y_1, \cdots, Y_N \mid \mu, \sigma \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma).
    \end{equation}
    
    \pause
    
    - The posterior distribution: 
    \begin{equation}
    \mu \mid Y_1, \cdots, Y_N,\textcolor{red}{\phi} \sim \textrm{Normal}\left(\frac{\phi_0 \mu_0 + N\phi\bar{Y} }{\phi_0 + N\phi}, \sqrt{\frac{1}{\phi_0 + N \phi}}\right),
    \end{equation}
    where $\phi = \frac{1}{\sigma^2}$ (and $\phi_0 = \frac{1}{\sigma_0^2}$), the precision. \textcolor{red}{Since $\sigma$ (and $\sigma_0$) is known, $\phi$ (and $\phi_0$) is known too.}


## Normal conjugate prior cont'd

- The posterior distribution: 
    \begin{equation}
    \mu \mid Y_1, \cdots, Y_N,\textcolor{red}{\phi} \sim \textrm{Normal}\left(\frac{\phi_0 \mu_0 + N\phi\bar{Y} }{\phi_0 + N\phi}, \sqrt{\frac{1}{\phi_0 + N \phi}}\right).
    \label{eq:NormalConjugate}
    \end{equation}
    
- We can then use the \texttt{rnorm()} R function to sample posterior draws of $\mu$ from Equation (\ref{eq:NormalConjugate}). \textcolor{red}{Known quantities: $\phi_0, \mu_0, N, \bar{Y}, \phi$}



## Example on log(Total Expenditure)


- Prior for $\mu$ is $\mu \sim \textrm{Normal}(5, 1)$, i.e. $\mu_0 = 5, \phi_0 = 1$

- Our log(Total Expenditure): $N = 6208$, $\bar{Y} = 8.75$

- Assume $\phi = 1.25$, i.e. $\sigma = \sqrt{1/1.25}$

- Use these quantities to obtain posterior for $\mu$:
\begin{equation}
    \mu \mid Y_1, \cdots, Y_N, \phi \sim \textrm{Normal}\left(\frac{\phi_0 \mu_0 + N\phi\bar{Y} }{\phi_0 + N\phi}, \sqrt{\frac{1}{\phi_0 + N \phi}}\right).
    \end{equation}


## Posterior for $\mu$

```{r size = "footnotesize"}
mu_0 <- 5
sigma_0 <- 1
phi_0 <- 1/sigma_0^2
ybar <- mean(CEsample$LogTotalExpLastQ)
phi <- 1.25
n <- dim(CEsample)[1]
mu_n <- (phi_0*mu_0+n*ybar*phi)/(phi_0+n*phi)
sd_n <- sqrt(1/(phi_0+n*phi))

mu_n
sd_n
```

## Posterior for $\mu$ cont'd

```{r fig.height = 3, fig.width = 5, fig.align = "center", size = "footnotesize", echo = FALSE}
par1 <- c(mu_0, sigma_0)
par2 <- c(mu_n, sd_n)
ggplot(data.frame(x = c(0, 25)), aes(x)) +
  stat_function(fun = dnorm, size = 1,
                linetype = "dashed",
                color = crcblue,
                args = list(mean = par1[1], sd = par1[2])) +
  stat_function(fun = dnorm, size = 1,
                color = crcblue,
                args = list(mean = par2[1], sd = par2[2])) +
  increasefont() +
  xlab(expression(mu))  +
  ylab("") +
  annotate(geom = "text", x = 5, y = 2,
           label = "Prior", size = 5) +
  annotate(geom = "text", x = 12, y = 20,
           label = "Posterior", size = 5)
```

## Bayesian inferences: hypothesis testing

```{r size = "footnotesize"}
mu_0 <- 5
set.seed(123)
S <- 1000
mu_post <- rnorm(S, mean = mu_n, sd = sd_n)
```


## Bayesian inferences: credible interval


## Bayesian inferences: prediction

## Using Stan


# Conjugate prior and posterior inferences for $\sigma$ 


## If only standard deviation $\sigma$ is unknown

- Special case: \textcolor{red}{$\mu$ is known, $\sigma$ is unknown}.

- There is only one parameter $\mu$ in $Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma)$.

- The Bayesian inference procedure simplifies to:
    - The data model/sampling density for $n$ observations with \textcolor{red}{$\mu$ known}:
    $$ 
    Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma).
    $$
    
    - The likelihood is in terms of unknown parameter $\sigma$:
    \begin{equation}
    f(y_1, \cdots, y_n) = L(\sigma).
    \end{equation}
    
    - Need a prior distribution for $\sigma$:
    \begin{equation}
    \pi(\sigma \mid \textcolor{red}{\mu}).
    \end{equation}
    
    - Bayes' rule will help us derive a posterior for $\sigma$:
    \begin{equation}
    \pi(\sigma \mid y_1, \cdots, y_n, \textcolor{red}{\mu}).
    \end{equation}


## If only standard deviation $\sigma$ is unknown: Gamma conjugate prior for $1/\sigma^2$

- For this special case, Gamma prior for $1/\sigma^2$ is a conjugate prior:
    - The prior distribution:
    \begin{equation}
    1/\sigma^2 \mid \textcolor{red}{\mu} \sim \textrm{Gamma}(\alpha, \beta).
    \end{equation}

    - The sampling density: 
    \begin{equation}
    y_1, \cdots, y_n \mid \mu, \sigma \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma).
    \end{equation}
    
    \pause
    
    - The posterior distribution: 
    \begin{equation}
    1/\sigma^2 \mid y_1, \cdots, y_n,\textcolor{red}{\mu} \sim \textrm{Gamma} \left(\alpha + \frac{n}{2}, \beta +                \frac{1}{2}\sum_{i=1}^{n}(y_i - \mu)^2 \right)
    \label{eq:GammaConjugate}
    \end{equation}
    
\pause

- We can then use \texttt{rgamma()} R function to sample posterior draws of $\sigma$ from Equation (\ref{eq:GammaConjugate}). \textcolor{red}{Known quantities: $\alpha, n, \beta, \{y_i\}, \mu$}


## Simulate posterior draws of $\sigma$

```{r size = "footnotesize"}
alpha <- 1
beta <- 1
mu <- 8
n <- dim(CEsample)[1]
alpha_n <- alpha+n/2
beta_n <- beta+1/2*sum((CEsample$LogTotalExpLastQ-mu)^2)

set.seed(123)
S <- 1000
invsigma2_post <- rgamma(S, shape=alpha_n, rate=beta_n)
df <- as.data.frame(invsigma2_post)
```


## Simulate posterior draws of $\sigma$ cont'd

```{r fig.height = 2, fig.width = 3.5, fig.align = "center", size = "footnotesize"}
ggplot(data = df, aes(invsigma2_post)) + 
  geom_density(color = crcblue, size = 1) + 
  labs(title = "Posterior density") +
  xlab(expression(1/sigma^2)) + 
  theme_grey(base_size = 8, base_family = "") 
```



# Recap

## Recap

- Bayesian inference procedure:
    - Step 1: express an opinion about the location of mean $\mu$ and standard deviation $\sigma$ (or precision $\phi$) before sampling (prior).
    - Step 2: take the sample (data/likelihood).
    - Step 3: use Bayes' rule to sharpen and update the previous opinion about $\mu$ and $\sigma$ (or precision $\phi$) given the information from the sample (posterior).
    
\pause

- For Normal data/likelihood, Normal distributions are conjugate priors for $\mu$, and Gamma distributions are conjugate priors for $\phi$.

\pause

- Bayesian inference
    - Bayesian hypothesis testing
    - Bayesian credible interval
    - Bayesian prediction
    - Posterior predictive checking
    
\pause

- What if we want to use a different prior for $\mu$? What if both $\mu$ and $\sigma$ are unknown?

